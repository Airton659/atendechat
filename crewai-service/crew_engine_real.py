# crew_engine_real.py - Motor CrewAI COMPLETO com logging no backend e Knowledge Base

from typing import Dict, Any, List, Optional
import time
import os
import requests
import unicodedata
from datetime import datetime, timedelta
import json
from crewai import Agent, Task, Crew, Process
from langchain_google_vertexai import ChatVertexAI
from simple_knowledge_service import get_knowledge_service
# from claude_validator import ClaudeValidator  # DESABILITADO

BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")

class RealCrewEngine:
    """Motor CrewAI completo com suporte a sequential, hierarchical, manager, logging e Knowledge Base"""

    def __init__(self):
        print("üöÄ Inicializando RealCrewEngine...")
        self.llm = None
        self.knowledge_service = get_knowledge_service()
        # self.claude_validator = None  # DESABILITADO
        self._initialize_llm()
        # self._initialize_claude_validator()  # DESABILITADO
        self.tools = {}  # Tools desabilitadas

    def _initialize_claude_validator(self):
        """DESABILITADO - Validator n√£o ser√° usado"""
        print("‚ö†Ô∏è  VALIDA√á√ÉO CLAUDE DESABILITADA - Sistema n√£o valida respostas")
        """DESABILITADO - Validator n√£o ser√° usado"""
        pass

    def _initialize_llm(self):
        """Inicializa o modelo Vertex AI padr√£o"""
        try:
            if 'OPENAI_API_KEY' in os.environ:
                del os.environ['OPENAI_API_KEY']
            
            self.llm = ChatVertexAI(
                model="gemini-2.0-flash-lite",
                project=os.getenv("GOOGLE_CLOUD_PROJECT"),
                location=os.getenv("GOOGLE_CLOUD_LOCATION"),
                temperature=0.7,
                max_output_tokens=1024,
            )
            print("‚úÖ Vertex AI (gemini-2.0-flash-lite) inicializado com sucesso!")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao inicializar Vertex AI: {e}")
            self.llm = None

    def _save_log_to_backend(self, log_data: Dict[str, Any]):
        """Salva log no backend"""
        try:
            response = requests.post(
                f"{BACKEND_URL}/agent-logs",
                json=log_data,
                timeout=5
            )
            if response.status_code == 201:
                print(f"‚úÖ Log salvo no backend (ID: {response.json().get('log', {}).get('id')})")
            else:
                print(f"‚ö†Ô∏è Erro ao salvar log: {response.status_code}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao conectar com backend para salvar log: {e}")

    def _get_llm_for_team(self, team_config: Dict[str, Any]) -> ChatVertexAI:
        """Cria LLM customizado baseado nas configura√ß√µes da equipe"""
        temperature = team_config.get('temperature', 0.7)
        model = "gemini-2.0-flash-lite"
        
        if team_config.get('processType') == 'hierarchical' and team_config.get('managerLLM'):
            model = team_config['managerLLM']
        
        try:
            llm = ChatVertexAI(
                model=model,
                project=os.getenv("GOOGLE_CLOUD_PROJECT"),
                location=os.getenv("GOOGLE_CLOUD_LOCATION"),
                temperature=temperature,
                max_output_tokens=1024,
            )
            print(f"‚úÖ LLM customizado criado: {model}, temperature={temperature}")
            return llm
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao criar LLM customizado: {e}, usando padr√£o")
            return self.llm

    def _normalize_text(self, text: str) -> str:
        """Remove acentos e normaliza texto para compara√ß√£o"""
        # Normaliza para NFD (separa caracteres base de acentos)
        nfd = unicodedata.normalize('NFD', text)
        # Remove acentos (categoria 'Mn' = Nonspacing Mark)
        return ''.join(char for char in nfd if unicodedata.category(char) != 'Mn').lower()

    def _get_agent_files(self, agent_id: int) -> List[Dict[str, Any]]:
        """Busca arquivos dispon√≠veis para o agente enviar via WhatsApp"""
        try:
            response = requests.get(
                f"{BACKEND_URL}/agent-files/agent/{agent_id}",
                timeout=3
            )

            if response.status_code == 200:
                files = response.json()
                print(f"üìé {len(files)} arquivos dispon√≠veis para agente {agent_id}")
                return files
            else:
                print(f"‚ö†Ô∏è Erro ao buscar arquivos: {response.status_code}")
                return []
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao buscar arquivos do agente: {e}")
            return []

    def _get_relevant_training_examples(self, agent_id: int, limit: int = 5) -> List[Dict[str, Any]]:
        """Busca exemplos de treinamento relevantes para few-shot learning"""
        try:
            response = requests.get(
                f"{BACKEND_URL}/agent-training-examples/relevant/{agent_id}",
                params={"limit": limit},
                timeout=3
            )

            if response.status_code == 200:
                examples = response.json().get('examples', [])
                print(f"‚úÖ {len(examples)} exemplos de treinamento carregados para agente {agent_id}")
                return examples
            else:
                print(f"‚ö†Ô∏è Erro ao buscar exemplos: {response.status_code}")
                return []
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao buscar exemplos de treinamento: {e}")
            return []

    def _format_training_examples_for_prompt(self, examples: List[Dict[str, Any]]) -> str:
        """Formata exemplos de treinamento para o prompt (Few-Shot Learning)

        Sistema de Prioridades:
        - Prioridade 10: CR√çTICO - Copiar EXATAMENTE
        - Prioridade 8-9: MUITO IMPORTANTE - Seguir DE PERTO
        - Prioridade 5-7: IMPORTANTE - APRENDER padr√£o e ADAPTAR
        - Prioridade 0-4: REFER√äNCIA - Inspira√ß√£o geral
        """
        if not examples:
            return ""

        prompt_parts = []
        prompt_parts.append("\n\n**üìö EXEMPLOS DE RESPOSTAS APROVADAS (Few-Shot Learning):**")
        prompt_parts.append("\nEstes s√£o exemplos reais de como voc√™ deve (ou n√£o deve) responder:\n")

        for idx, example in enumerate(examples, 1):
            feedback_type = example.get('feedbackType', 'approved')
            user_msg = example.get('userMessage', '')
            agent_resp = example.get('agentResponse', '')
            corrected_resp = example.get('correctedResponse')
            notes = example.get('feedbackNotes', '')
            priority = example.get('priority', 5)  # Default 5 se n√£o tiver

            prompt_parts.append(f"\n**Exemplo {idx}:**")
            prompt_parts.append(f"Cliente: {user_msg}")

            if feedback_type == "corrected":
                # Mostrar resposta errada e correta
                prompt_parts.append(f"‚ùå Resposta ERRADA: {agent_resp}")
                prompt_parts.append(f"‚úÖ Resposta CORRETA: {corrected_resp}")
                if notes:
                    prompt_parts.append(f"üí° Motivo da corre√ß√£o: {notes}")
            elif feedback_type == "approved":
                # Exemplo de resposta boa
                prompt_parts.append(f"‚úÖ Resposta APROVADA: {agent_resp}")
                if notes:
                    prompt_parts.append(f"üí° Nota: {notes}")

            # ADICIONAR INSTRU√á√ÉO BASEADA NA PRIORIDADE
            if priority >= 10:
                prompt_parts.append("üî¥ **PRIORIDADE CR√çTICA (10)**: Copie EXATAMENTE este formato, estrutura e tom. Este √© um padr√£o obrigat√≥rio.")
            elif priority >= 8:
                prompt_parts.append("üü† **PRIORIDADE MUITO ALTA (8-9)**: Siga este padr√£o MUITO DE PERTO. Se houver outros exemplos com esta prioridade, COMBINE as regras de todos.")
            elif priority >= 5:
                prompt_parts.append("üü° **PRIORIDADE ALTA (5-7)**: APRENDA o padr√£o (tom, objetividade, n√≠vel de detalhe) e ADAPTE ao contexto atual. N√ÉO copie literalmente.")
            else:
                prompt_parts.append("üü¢ **PRIORIDADE BAIXA (0-4)**: Use como inspira√ß√£o geral. Voc√™ tem liberdade para adaptar.")

        # INSTRU√á√ïES GERAIS SOBRE COMO USAR OS EXEMPLOS
        prompt_parts.append("\n‚ö†Ô∏è INSTRU√á√ïES IMPORTANTES - COMO USAR ESTES EXEMPLOS:")
        prompt_parts.append("")
        prompt_parts.append("üî¥ **PRIORIDADE 10 (CR√çTICO)**:")
        prompt_parts.append("   - Copie EXATAMENTE a estrutura, tom e formato mostrado")
        prompt_parts.append("   - Estes s√£o padr√µes obrigat√≥rios que N√ÉO devem ser alterados")
        prompt_parts.append("   - Use para: pol√≠ticas fixas, avisos legais, procedimentos obrigat√≥rios")
        prompt_parts.append("")
        prompt_parts.append("üü† **PRIORIDADE 8-9 (MUITO IMPORTANTE)**:")
        prompt_parts.append("   - Siga MUITO DE PERTO o padr√£o mostrado")
        prompt_parts.append("   - Mantenha a estrutura e tom")
        prompt_parts.append("   - Se houver m√∫ltiplos exemplos desta prioridade, COMBINE os conhecimentos")
        prompt_parts.append("")
        prompt_parts.append("üü° **PRIORIDADE 5-7 (IMPORTANTE)** ‚Üê PADR√ÉO MAIS COMUM:")
        prompt_parts.append("   - APRENDA o padr√£o: tom de voz, n√≠vel de detalhe, objetividade, estrutura")
        prompt_parts.append("   - ADAPTE ao contexto atual da conversa")
        prompt_parts.append("   - N√ÉO copie palavra por palavra - seja natural e contextual")
        prompt_parts.append("   - Mantenha o ESTILO aprendido mas ajuste o CONTE√öDO ao contexto")
        prompt_parts.append("")
        prompt_parts.append("üü¢ **PRIORIDADE 0-4 (REFER√äNCIA)**:")
        prompt_parts.append("   - Use apenas como inspira√ß√£o geral")
        prompt_parts.append("   - Voc√™ tem liberdade para adaptar como achar melhor")
        prompt_parts.append("")
        prompt_parts.append("‚ö†Ô∏è **REGRA GERAL**: Preste aten√ß√£o nos exemplos marcados como ‚ùå ERRADOS - NUNCA fa√ßa igual a eles!")

        return "\n".join(prompt_parts)

    def _select_agent_by_keywords(self, message: str, agents: List[Dict[str, Any]], default_agent_id: Optional[int] = None, conversation_history: List[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """
        Seleciona o agente mais apropriado baseado nas palavras-chave.
        MANT√âM o agente atual se j√° houver contexto de conversa em andamento.
        """
        # üîÑ NOVO: Verificar se h√° um agente j√° atendendo esta conversa
        # REGRA: Se h√° hist√≥rico com assistant reply, SEMPRE manter o mesmo agente
        # A MENOS QUE usu√°rio explicitamente mencione outro agente por nome
        if conversation_history and len(conversation_history) >= 2:
            # Verificar se h√° resposta do assistant no hist√≥rico
            has_assistant_reply = any(msg.get('role') == 'assistant' for msg in conversation_history)

            if has_assistant_reply:
                # Encontrar qual agente tem maior score com a mensagem atual
                message_normalized = self._normalize_text(message)
                best_agent = None
                best_score = 0

                for agent in agents:
                    if not agent.get('isActive', True):
                        continue

                    keywords = agent.get('keywords', [])
                    if not keywords:
                        continue

                    score = 0
                    for keyword in keywords:
                        keyword_normalized = self._normalize_text(keyword)
                        if keyword_normalized in message_normalized:
                            score += 1

                    if score > best_score:
                        best_score = score
                        best_agent = agent

                # Se encontrou algum agente (mesmo com score 0), retorn√°-lo
                # Isso mant√©m o agente atual a menos que n√£o haja nenhum ativo
                if best_agent:
                    print(f"üîÑ Mantendo agente por contexto: '{best_agent['name']}' (score: {best_score})")
                    return best_agent

                # Se n√£o encontrou nenhum agente com keywords, pegar o primeiro ativo
                for agent in agents:
                    if agent.get('isActive', True):
                        print(f"üîÑ Mantendo primeiro agente ativo por contexto: '{agent['name']}'")
                        return agent

        # Se n√£o h√° contexto ou precisa trocar, usar sele√ß√£o normal por keywords
        message_normalized = self._normalize_text(message)

        print("\n" + "="*60)
        print("üîç MATCHING DE KEYWORDS - DEBUG DETALHADO")
        print("="*60)
        print(f"üìù Mensagem original: '{message}'")
        print(f"üìù Mensagem normalizada: '{message_normalized}'")
        print("="*60)

        agent_scores = []

        for agent in agents:
            agent_name = agent.get('name', 'Unknown')

            if not agent.get('isActive', True):
                print(f"‚è≠Ô∏è  Agente '{agent_name}' est√° INATIVO, pulando...")
                continue

            score = 0
            keywords = agent.get('keywords', [])
            matched_keywords = []

            print(f"\nü§ñ Testando agente: {agent_name}")
            print(f"   Keywords configuradas: {keywords}")

            if keywords:
                for keyword in keywords:
                    keyword_normalized = self._normalize_text(keyword)
                    print(f"   üîë Keyword '{keyword}' ‚Üí normalizada: '{keyword_normalized}'")

                    if keyword_normalized in message_normalized:
                        score += 1
                        matched_keywords.append(keyword)
                        print(f"      ‚úÖ MATCH! '{keyword_normalized}' encontrado em '{message_normalized}'")
                    else:
                        print(f"      ‚ùå N√£o encontrado")
            else:
                print(f"   ‚ö†Ô∏è  Agente sem keywords configuradas")

            if score > 0:
                agent_scores.append((agent, score))
                print(f"   üìä Score final: {score} (keywords matched: {matched_keywords})")
            else:
                print(f"   üìä Score final: 0 (nenhuma keyword matched)")

        print("\n" + "="*60)
        print("üìä RESULTADO DO MATCHING")
        print("="*60)

        if agent_scores:
            agent_scores.sort(key=lambda x: x[1], reverse=True)
            selected = agent_scores[0][0]
            print(f"‚úÖ AGENTE SELECIONADO: {selected.get('name')} (score: {agent_scores[0][1]})")
            print(f"   Total de agentes com match: {len(agent_scores)}")
            if len(agent_scores) > 1:
                print(f"   Outros candidatos:")
                for agent, score in agent_scores[1:]:
                    print(f"      - {agent.get('name')}: score {score}")
            print("="*60 + "\n")
            return selected

        print("‚ö†Ô∏è  NENHUMA KEYWORD MATCHED - Usando agente padr√£o")
        for agent in agents:
            if agent.get('isActive', True):
                print(f"‚úÖ AGENTE PADR√ÉO SELECIONADO: {agent.get('name')}")
                print("="*60 + "\n")
                return agent

        print("‚ùå NENHUM AGENTE ATIVO ENCONTRADO")
        print("="*60 + "\n")
        return None

    def _build_full_prompt(self, message: str, agent_data: Dict[str, Any], conversation_history: List[Dict[str, Any]], knowledge_context: Optional[str] = None) -> tuple[str, List[Dict[str, Any]]]:
        """Constr√≥i o prompt completo com TODAS as configura√ß√µes do agente + Knowledge Base + Tool Context

        Returns:
            tuple: (prompt_completo, training_examples_usados)
        """

        name = agent_data.get('name', 'Agente')
        role = agent_data.get('function', 'Assistente de atendimento')
        objective = agent_data.get('objetivo', 'Ajudar o cliente')
        backstory = agent_data.get('backstory', '')
        custom_instructions = agent_data.get('customInstructions', '')
        persona = agent_data.get('persona', '')
        do_list = agent_data.get('doList', [])
        dont_list = agent_data.get('dontList', [])

        print("\n" + "="*60)
        print("üìã CONFIGURA√á√ÉO DO AGENTE:")
        print("="*60)
        print(f"üë§ Nome: {name}")
        print(f"üíº Fun√ß√£o: {role}")
        print(f"üéØ Objetivo: {objective}")
        print(f"‚úÖ DO List ({len(do_list)} itens): {do_list}")
        print(f"‚ùå DONT List ({len(dont_list)} itens): {dont_list}")
        if knowledge_context:
            print(f"üìö Knowledge Base: SIM ({len(knowledge_context)} chars)")
        print("="*60 + "\n")

        # Buscar exemplos de treinamento (Few-Shot Learning)
        training_examples = []
        agent_id = agent_data.get('id')
        if agent_id:
            training_examples = self._get_relevant_training_examples(agent_id, limit=5)
            if training_examples:
                print(f"üéì {len(training_examples)} exemplos de treinamento ser√£o usados para Few-Shot Learning")
                for idx, ex in enumerate(training_examples, 1):
                    print(f"   Exemplo {idx}: {ex.get('feedbackType')} - Priority {ex.get('priority')}")

        prompt_parts = []
        prompt_parts.append(f"Voc√™ √© {name}, {role}.")
        prompt_parts.append(f"\nSeu objetivo √©: {objective}")

        # ADICIONAR HIST√ìRICO LOGO AP√ìS OBJETIVO (ANTES DAS INSTRU√á√ïES)
        # Isso garante que o LLM veja o contexto da conversa ANTES das regras
        if conversation_history:
            print(f"\nüí¨ HIST√ìRICO DA CONVERSA: {len(conversation_history)} mensagens")
            for idx, msg in enumerate(conversation_history, 1):
                role_label = msg.get('role', 'Cliente')
                body = msg.get('body', '')
                print(f"   [{idx}] {role_label}: {body[:80]}{'...' if len(body) > 80 else ''}")

            prompt_parts.append("\n\n**üìú HIST√ìRICO DA CONVERSA AT√â AGORA:**")
            for msg in conversation_history:
                role_label = msg.get('role', 'Cliente')
                prompt_parts.append(f"{role_label}: {msg.get('body', '')}")
            prompt_parts.append("\n---\n")

        if backstory:
            prompt_parts.append(f"\n\n**SUA HIST√ìRIA E CONTEXTO:**\n{backstory}")

        if persona:
            prompt_parts.append(f"\n\n**SUA PERSONA:**\n{persona}")

        if custom_instructions:
            prompt_parts.append(f"\n\n**INSTRU√á√ïES ESPECIAIS:**\n{custom_instructions}")

        # ADICIONAR KNOWLEDGE BASE LOGO AP√ìS INSTRU√á√ïES
        if knowledge_context:
            prompt_parts.append(f"\n\n**üìö BASE DE CONHECIMENTO - INFORMA√á√ïES OFICIAIS:**")
            prompt_parts.append(knowledge_context)
            prompt_parts.append("\nüî• REGRA CR√çTICA - PRIORIDADE DA BASE DE CONHECIMENTO:")
            prompt_parts.append("1. SE a pergunta do cliente puder ser respondida com informa√ß√µes da Base de Conhecimento acima, voc√™ DEVE usar essas informa√ß√µes")
            prompt_parts.append("2. N√ÉO fale sobre voc√™ mesmo (suas fun√ß√µes/responsabilidades como agente) se a pergunta for sobre algo que est√° na Base de Conhecimento")
            prompt_parts.append("3. A Base de Conhecimento cont√©m informa√ß√µes OFICIAIS e AUTORITATIVAS - sempre priorize-a")
            prompt_parts.append("4. N√ÉO invente, N√ÉO assuma, N√ÉO adicione informa√ß√µes que n√£o estejam explicitamente na base")
            prompt_parts.append("5. Se N√ÉO houver informa√ß√£o relevante na base, a√≠ sim responda normalmente com base na sua fun√ß√£o")
            prompt_parts.append("6. NUNCA mencione recursos ou funcionalidades que voc√™ N√ÉO possui (ex: enviar imagens, fotos, v√≠deos, links)")
            prompt_parts.append("7. Voc√™ APENAS pode enviar arquivos usando [SEND_FILE:id] se o arquivo estiver listado na se√ß√£o 'ARQUIVOS DISPON√çVEIS'")
            prompt_parts.append("8. N√ÉO use tags ou c√≥digos falsos como [SEND_IMAGE:...], [SEND_PHOTO:...] - eles N√ÉO funcionam")
            prompt_parts.append("")
            prompt_parts.append("üéØ REGRA CR√çTICA - CONTEXTO CONVERSACIONAL E PRONOMES:")
            prompt_parts.append("6. MANTENHA O CONTEXTO: Se o cliente perguntou sobre uma pessoa/entidade espec√≠fica (ex: 'Dr. Ricardo', 'produto X', 'servi√ßo Y'), guarde essa informa√ß√£o")
            prompt_parts.append("7. RESOLVA PRONOMES: Quando o cliente usar pronomes como 'ele', 'ela', 'isso', 'esse', 'essa', 'aquele', refira-se √† √öLTIMA entidade mencionada na conversa")
            prompt_parts.append("8. FILTRE INFORMA√á√ïES: Se o cliente perguntar 'quais exames ELE realiza?' e estava falando do Dr. Ricardo, responda APENAS sobre o Dr. Ricardo, N√ÉO liste todos os m√©dicos")
            prompt_parts.append("9. SEJA CONTEXTUAL: Analise o hist√≥rico da conversa para entender sobre QUEM/O QUE o cliente est√° perguntando")
            prompt_parts.append("10. EXEMPLO PR√ÅTICO:")
            prompt_parts.append("    Cliente: 'Que dia o Dr. Ricardo atende?'")
            prompt_parts.append("    Voc√™: 'Dr. Ricardo atende ter√ßas-feiras'")
            prompt_parts.append("    Cliente: 'Quais exames ele realiza?' ‚Üê 'ele' = Dr. Ricardo")
            prompt_parts.append("    Voc√™: 'Dr. Ricardo realiza EEG e Resson√¢ncia Magn√©tica' ‚Üê APENAS Dr. Ricardo, N√ÉO todos os m√©dicos!\n")

        # ADICIONAR EXEMPLOS DE TREINAMENTO (Few-Shot Learning)
        if training_examples:
            examples_formatted = self._format_training_examples_for_prompt(training_examples)
            prompt_parts.append(examples_formatted)

        # ADICIONAR ARQUIVOS DISPON√çVEIS PARA ENVIO
        if agent_id:
            agent_files = self._get_agent_files(agent_id)
            if agent_files:
                prompt_parts.append("\n\n**üìé ARQUIVOS DISPON√çVEIS PARA ENVIO:**")
                prompt_parts.append("Voc√™ tem os seguintes arquivos que pode enviar ao cliente quando solicitado:")
                for file in agent_files:
                    file_desc = file.get('description') or file.get('originalName', 'Arquivo')
                    file_type = file.get('fileType', 'arquivo').upper()
                    prompt_parts.append(f"- [SEND_FILE:{file.get('id')}] {file_desc} ({file_type})")
                prompt_parts.append("\n**COMO ENVIAR ARQUIVOS:**")
                prompt_parts.append("- Quando o cliente pedir um arquivo (card√°pio, tabela de pre√ßos, documento, etc), inclua o c√≥digo [SEND_FILE:id] na sua resposta")
                prompt_parts.append("- Exemplo: 'Claro! Vou te enviar o card√°pio agora. [SEND_FILE:1]'")
                prompt_parts.append("- O arquivo ser√° enviado automaticamente pelo sistema")
                prompt_parts.append("- SEMPRE responda com uma frase natural ANTES do c√≥digo [SEND_FILE:id]")
                prompt_parts.append("- Voc√™ pode enviar m√∫ltiplos arquivos se necess√°rio: [SEND_FILE:1] [SEND_FILE:2]")

        if do_list:
            prompt_parts.append("\n\n**VOC√ä DEVE:**")
            for item in do_list:
                prompt_parts.append(f"- {item}")

        if dont_list:
            prompt_parts.append("\n\n**‚õî VOC√ä N√ÉO DEVE (PROIBIDO - NUNCA FA√áA ISSO):**")
            for item in dont_list:
                prompt_parts.append(f"‚ùå {item}")
            prompt_parts.append("\n‚ö†Ô∏è ATEN√á√ÉO: As regras acima s√£o OBRIGAT√ìRIAS e DEVEM ser seguidas em TODAS as respostas, sem exce√ß√£o.")

        prompt_parts.append(f"\n\n**MENSAGEM ATUAL DO CLIENTE:**\n{message}")

        prompt_parts.append("\n\n**SUA RESPOSTA:**")

        full_prompt = "\n".join(prompt_parts)

        print("PROMPT COMPLETO:")
        print(full_prompt[:2000])

        return full_prompt, training_examples

    def _validate_response_against_config(self, response: str, agent_data: Dict[str, Any], llm: ChatVertexAI, conversation_history: List[Dict[str, str]] = None) -> str:
        """
        Validacao 100% generica usando Claude Haiku (prim√°rio) ou Gemini Free (fallback)
        Claude: 95%+ acur√°cia, $0.0002-0.0006 por valida√ß√£o
        Fallback: Gemini Free se Claude indispon√≠vel ou limite di√°rio atingido
        """
        if conversation_history is None:
            conversation_history = []

        # Tentar usar Claude Validator primeiro
        if self.claude_validator:
            try:
                result = self.claude_validator.validate_response(response, agent_data, conversation_history)

                # Se usou Claude com sucesso
                if result["method"] == "claude":
                    return result["corrected_response"]

                # Se caiu em fallback (limite di√°rio, erro, etc), usar Gemini abaixo
                print(f"‚ö†Ô∏è  Claude fallback: {result['reason']}")
                print("‚ö†Ô∏è  Usando valida√ß√£o Gemini Free...")

            except Exception as e:
                print(f"‚ùå Erro ao usar Claude Validator: {e}")
                print("‚ö†Ô∏è  Usando valida√ß√£o Gemini Free (fallback)...")

        # Fallback: Valida√ß√£o com Gemini Free (m√©todo original)
        dont_list = agent_data.get("dontList", [])
        do_list = agent_data.get("doList", [])
        persona = agent_data.get("persona", "")
        custom_instructions = agent_data.get("customInstructions", "")

        # Se nao tem nenhuma regra, nao precisa validar
        if not dont_list and not do_list and not persona and not custom_instructions:
            return response

        print("\n" + "="*60)
        print("VALIDACAO GEMINI FREE (FALLBACK)")
        print("="*60 + "\n")

        # Construir prompt de validacao
        validation_parts = []
        validation_parts.append("Voce e um validador. Analise se a resposta respeita as regras:\n\n")
        validation_parts.append(f"RESPOSTA:\n{response}\n\n")
        validation_parts.append("REGRAS:\n")

        if do_list:
            validation_parts.append("DO List: " + ", ".join(do_list) + "\n")
        if dont_list:
            validation_parts.append("DONT List: " + ", ".join(dont_list) + "\n")
        if persona:
            validation_parts.append(f"Persona: {persona}\n")
        if custom_instructions:
            validation_parts.append(f"Instrucoes: {custom_instructions}\n")

        validation_parts.append("\nRESPONDA: OK ou VIOLACAO: [explicacao]")
        validation_prompt = "".join(validation_parts)

        try:
            from langchain_core.messages import HumanMessage
            validation_response = llm.invoke([HumanMessage(content=validation_prompt)])
            validation_text = validation_response.content.strip()

            print(f"Resultado: {validation_text}\n")

            if "VIOLACAO" in validation_text.upper():
                print("Violacao detectada! Pedindo reescrita...\n")

                rewrite_parts = []
                rewrite_parts.append(f"A resposta abaixo violou regras: {validation_text}\n\n")
                rewrite_parts.append(f"RESPOSTA ORIGINAL:\n{response}\n\n")
                rewrite_parts.append("REESCREVA respeitando:\n")

                if do_list:
                    rewrite_parts.append("DO: " + ", ".join(do_list) + "\n")
                if dont_list:
                    rewrite_parts.append("DONT: " + ", ".join(dont_list) + "\n")
                if persona:
                    rewrite_parts.append(f"Persona: {persona}\n")
                if custom_instructions:
                    rewrite_parts.append(f"Instrucoes: {custom_instructions}\n")

                rewrite_parts.append("\nResposta corrigida:")
                rewrite_prompt = "".join(rewrite_parts)
                rewrite_response = llm.invoke([HumanMessage(content=rewrite_prompt)])
                corrected = rewrite_response.content.strip()

                print(f"Resposta corrigida:\n{corrected}\n" + "="*60 + "\n")
                return corrected
            else:
                print("OK - regras respeitadas\n" + "="*60 + "\n")
                return response

        except Exception as e:
            print(f"Erro na validacao Gemini: {e}")
            return response


    def _create_crewai_agent(self, agent_data: Dict[str, Any], llm: ChatVertexAI, is_manager: bool = False) -> Agent:
        """
        Converte configura√ß√£o de agente para objeto CrewAI Agent
        
        Args:
            agent_data: Dados do agente (name, function, objective, backstory, etc)
            llm: Modelo LLM a ser usado (Vertex AI)
            is_manager: Se True, adiciona capacidade de delega√ß√£o
        """
        # Construir objetivo completo com instru√ß√µes customizadas
        full_goal = agent_data.get('objective', '')
        
        if agent_data.get('customInstructions'):
            full_goal += f"\n\nInstru√ß√µes especiais: {agent_data['customInstructions']}"
        
        # Adicionar DO List e DON'T List ao backstory
        backstory = agent_data.get('backstory', '')
        
        do_list = agent_data.get('doList', [])
        if do_list:
            backstory += "\n\nCoisas que VOC√ä DEVE fazer:"
            for item in do_list:
                if item.strip():
                    backstory += f"\n- {item}"
        
        dont_list = agent_data.get('dontList', [])
        if dont_list:
            backstory += "\n\nCoisas que voc√™ N√ÉO DEVE fazer (PROIBIDO):"
            for item in dont_list:
                if item.strip():
                    backstory += f"\n- {item}"
        
        if agent_data.get('persona'):
            backstory += f"\n\nPersona: {agent_data['persona']}"
        
        # IMPORTANTE: Remover OPENAI_API_KEY para for√ßar uso do Vertex AI
        import os
        if 'OPENAI_API_KEY' in os.environ:
            del os.environ['OPENAI_API_KEY']
        
        # Criar agente CrewAI com LLM Vertex AI expl√≠cito
        # IMPORTANTE: memory=False para evitar uso de OpenAI embeddings
        agent = Agent(
            role=agent_data.get('function', 'Assistente'),
            goal=full_goal,
            backstory=backstory,
            llm=llm,  # Vertex AI (Gemini) passado explicitamente
            verbose=True,
            allow_delegation=is_manager,  # Apenas manager pode delegar
            memory=False,  # Desabilita mem√≥ria interna do CrewAI (usamos nosso pr√≥prio hist√≥rico)
            embedder=None  # N√£o usar embedder (evita OpenAI)
        )
        
        # Armazenar metadados adicionais
        agent._original_data = agent_data
        
        print(f"   ‚úÖ Agente criado com LLM: {llm.model_name if hasattr(llm, 'model_name') else 'Vertex AI'}")
        
        return agent

    def _run_manual_hierarchical_delegation(
        self,
        message: str,
        manager_agent_data: Dict[str, Any],
        specialist_agents_data: List[Dict[str, Any]],
        conversation_history: List[Dict[str, Any]],
        llm: ChatVertexAI,
        knowledge_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Delega√ß√£o hier√°rquica MANUAL usando apenas Vertex AI (sem CrewAI framework)
        
        Fluxo:
        1. Manager analisa mensagem e decide qual especialista usar
        2. Especialista selecionado processa a mensagem
        3. Retorna resposta do especialista
        
        Args:
            message: Mensagem do cliente
            manager_agent_data: Dados do Manager Agent
            specialist_agents_data: Lista de especialistas dispon√≠veis
            conversation_history: Hist√≥rico da conversa
            llm: Modelo LLM Vertex AI
            knowledge_context: Contexto da KB (se houver)
        
        Returns:
            Dict com success, response, agent_used, delegation_info
        """
        try:
            print("\n" + "="*60)
            print("üéØ DELEGA√á√ÉO HIER√ÅRQUICA MANUAL - Vertex AI Only")
            print("="*60)
            print(f"Manager: {manager_agent_data.get('name')}")
            print(f"Especialistas dispon√≠veis: {len(specialist_agents_data)}")
            
            # 1. Preparar contexto dos especialistas para o Manager
            specialists_info = []
            for idx, spec in enumerate(specialist_agents_data, 1):
                spec_name = spec.get('name', 'Unknown')
                spec_function = spec.get('function', 'Unknown')
                spec_keywords = spec.get('keywords', [])
                spec_objective = spec.get('objective', '')[:150]
                
                specialists_info.append(
                    f"{idx}. {spec_name} ({spec_function})\n"
                    f"   Especialidades: {', '.join(spec_keywords)}\n"
                    f"   Objetivo: {spec_objective}..."
                )
                print(f"   {idx}. {spec_name} - Keywords: {spec_keywords}")
            
            specialists_context = "\n".join(specialists_info)
            
            # 2. Manager decide qual especialista usar (via Vertex AI)
            print("\nü§î Manager analisando mensagem para decidir delega√ß√£o...")
            
            delegation_prompt = f"""Voc√™ √© {manager_agent_data.get('name')}, {manager_agent_data.get('function')}.

ESPECIALISTAS DISPON√çVEIS:
{specialists_context}

MENSAGEM DO CLIENTE:
{message}

SUA TAREFA:
Analise a mensagem do cliente e decida qual especialista deve responder.

RESPONDA APENAS COM O N√öMERO do especialista (1, 2, 3, etc.) OU "0" se voc√™ mesmo deve responder.

üî• REGRAS DE DELEGA√á√ÉO:

1. VOC√ä (Manager) S√ì responde (0) se for:
   - Sauda√ß√£o gen√©rica SEM pedido espec√≠fico: "oi", "ol√°", "bom dia", "boa tarde"

2. SEMPRE DELEGUE (1, 2, 3...) para o especialista apropriado quando o cliente:
   - Fizer uma pergunta espec√≠fica
   - Pedir informa√ß√µes detalhadas
   - Solicitar algum servi√ßo/a√ß√£o
   - Mencionar palavras-chave que combinem com as especialidades dos especialistas

3. ANALISE O CONTEXTO DA PERGUNTA, N√ÉO OS NOMES:
   - ‚ö†Ô∏è IMPORTANTE: Se a mensagem mencionar o nome de um especialista (ex: "Ricardo", "Carlos"),
     N√ÉO delegue automaticamente para ele s√≥ por causa do nome
   - Analise o ASSUNTO/CONTEXTO da pergunta
   - Compare o ASSUNTO com as especialidades (palavras-chave) de cada especialista
   - Escolha o especialista cujas ESPECIALIDADES mais combinam com o ASSUNTO da pergunta
   - Exemplo: "Qual especialidade do Ricardo?" ‚Üí assunto √© "especialidade/informa√ß√£o" ‚Üí delegar para especialista de SUPORTE/D√öVIDAS, N√ÉO para Ricardo

4. PRIORIDADE DAS PALAVRAS-CHAVE:
   - Identifique o assunto principal da pergunta
   - Compare com as especialidades listadas acima
   - Se houver d√∫vida entre 2 especialistas, escolha o mais espec√≠fico

RESPONDA APENAS O N√öMERO (0, 1, 2, 3...), NADA MAIS."""

            from langchain_core.messages import HumanMessage
            delegation_response = llm.invoke([HumanMessage(content=delegation_prompt)])
            delegation_choice = delegation_response.content.strip()
            
            print(f"‚úÖ Manager decidiu: '{delegation_choice}'")
            
            # 3. Selecionar agente baseado na decis√£o
            try:
                choice_num = int(delegation_choice)
                
                if choice_num == 0:
                    # Manager responde diretamente
                    selected_agent_data = manager_agent_data
                    print(f"‚úÖ Manager vai responder diretamente")
                elif 1 <= choice_num <= len(specialist_agents_data):
                    # Delegar para especialista
                    selected_agent_data = specialist_agents_data[choice_num - 1]
                    print(f"‚úÖ Delegando para: {selected_agent_data.get('name')}")
                else:
                    # N√∫mero inv√°lido, usar Manager
                    print(f"‚ö†Ô∏è  N√∫mero inv√°lido ({choice_num}), Manager responde")
                    selected_agent_data = manager_agent_data
            except ValueError:
                # Resposta n√£o foi um n√∫mero, usar Manager
                print(f"‚ö†Ô∏è  Resposta n√£o num√©rica, Manager responde")
                selected_agent_data = manager_agent_data
            
            # 4. Especialista selecionado gera a resposta
            print(f"\nüöÄ Gerando resposta com {selected_agent_data.get('name')}...")

            response_text, prompt_used, training_examples_used = self._create_simple_response(
                message,
                selected_agent_data,
                conversation_history,
                llm,
                knowledge_context
            )

            print(f"‚úÖ Resposta gerada por {selected_agent_data.get('name')}")
            
            return {
                "success": True,
                "response": response_text,
                "agent_used": selected_agent_data.get('name'),
                "delegation_info": {
                    "manager": manager_agent_data.get('name'),
                    "manager_choice": delegation_choice,
                    "delegated_to": selected_agent_data.get('name'),
                    "specialists_available": len(specialist_agents_data),
                    "method": "manual_vertex_ai"
                }
            }
            
        except Exception as e:
            print(f"‚ùå Erro na delega√ß√£o manual: {e}")
            import traceback
            traceback.print_exc()
            
            # Fallback: Manager responde diretamente
            print("‚ö†Ô∏è  Fallback: Manager responde diretamente...")
            fallback_response, _, _ = self._create_simple_response(
                message,
                manager_agent_data,
                conversation_history,
                llm,
                knowledge_context
            )
            
            return {
                "success": True,
                "response": fallback_response,
                "agent_used": manager_agent_data.get('name'),
                "delegation_info": {
                    "manager": manager_agent_data.get('name'),
                    "error": str(e),
                    "fallback": True
                }
            }


    def _create_simple_response(self, message: str, agent_data: Dict[str, Any], conversation_history: List[Dict[str, Any]], llm: ChatVertexAI, knowledge_context: Optional[str] = None) -> tuple[str, str, List[Dict[str, Any]]]:
        """Gera resposta usando Vertex AI diretamente

        Returns:
            tuple: (validated_response, prompt_completo, training_examples_usados)
        """
        try:
            prompt, training_examples = self._build_full_prompt(message, agent_data, conversation_history, knowledge_context)

            from langchain_core.messages import HumanMessage
            response = llm.invoke([HumanMessage(content=prompt)])

            print("\n" + "="*60)
            print("üì• RESPOSTA RECEBIDA:")
            print("="*60)
            print(response.content)
            print("="*60 + "\n")

            # TEMPORARIAMENTE DESABILITADO - DEBUGANDO
            # Aplicar validacao generica (100% baseada na config da equipe)
            # validated_response = self._validate_response_against_config(response.content, agent_data, llm, conversation_history)
            # return validated_response, prompt, training_examples

            print("‚ö†Ô∏è VALIDA√á√ÉO TEMPORARIAMENTE DESABILITADA - DEBUGANDO")
            return response.content, prompt, training_examples

        except Exception as e:
            print(f"‚ùå Erro ao gerar resposta: {e}")
            import traceback
            traceback.print_exc()
            return "Ol√°! Como posso ajud√°-lo hoje?", "", []

    async def run_playground_crew(
        self,
        team_definition: Dict[str, Any],
        task: str,
        company_id: int,
        conversation_history: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Executa uma Crew TEMPOR√ÅRIA no modo Playground (n√£o salva logs no banco).
        Usado para testar e refinar prompts antes de salvar altera√ß√µes.

        Args:
            conversation_history: Lista de mensagens anteriores [{"role": "user"|"assistant", "content": "..."}]
        """
        if conversation_history is None:
            conversation_history = []
        print("\n" + "="*60)
        print("üß™ RUN PLAYGROUND CREW - Executando equipe tempor√°ria")
        print("="*60)

        # Capturar logs verbosos
        import io
        import sys
        log_capture = io.StringIO()
        original_stdout = sys.stdout

        success = False
        response_text = ""
        error_message = None
        agent_used = None
        execution_logs = ""

        try:
            # Extrair dados da defini√ß√£o tempor√°ria
            team_name = team_definition.get('name', 'Equipe Tempor√°ria')
            agents_data = team_definition.get('agents', [])
            process_type = team_definition.get('processType', 'sequential')
            temperature = team_definition.get('temperature', 0.7)

            print(f"Team: {team_name}")
            print(f"Process Type: {process_type}")
            print(f"Temperature: {temperature}")
            print(f"Agents: {len(agents_data)}")
            print(f"Task: {task}")
            print("="*60 + "\n")

            if not agents_data:
                raise ValueError("A equipe precisa ter pelo menos 1 agente")

            if not self.llm:
                raise ValueError("LLM n√£o inicializado")

            # Criar LLM customizado
            custom_llm = self._get_llm_for_team({
                'temperature': temperature,
                'processType': process_type,
                'managerLLM': team_definition.get('managerLLM')
            })

            # Converter hist√≥rico para o formato esperado ANTES de usar
            # IMPORTANTE: Usar "role" (n√£o "sender") para que _build_full_prompt pegue corretamente
            formatted_history = []
            if conversation_history:
                for msg in conversation_history:
                    if msg.get('role') == 'user':
                        formatted_history.append({"role": "Cliente", "body": msg.get('content', '')})
                    elif msg.get('role') == 'assistant':
                        formatted_history.append({"role": "Voc√™", "body": msg.get('content', '')})

            # Redirecionar stdout para log_capture ANTES de processar
            sys.stdout = log_capture

            # DECIS√ÉO: Hierarchical ou Sequential
            if process_type == 'hierarchical':
                # MODO HIERARCHICAL: Usar delega√ß√£o manual
                manager_agent_id = team_definition.get('managerAgentId')
                
                print(f"üîç DEBUG - team_definition keys: {team_definition.keys()}")
                print(f"üîç DEBUG - managerAgentId value: {manager_agent_id}")
                print(f"üîç DEBUG - managerAgentId type: {type(manager_agent_id)}")
                
                if not manager_agent_id:
                    print(f"‚ùå DEBUG - team_definition completo: {team_definition}")
                    raise ValueError("Modo hierarchical requer managerAgentId configurado")
                
                manager_agent_data = next((a for a in agents_data if a.get('id') == manager_agent_id), None)
                if not manager_agent_data:
                    raise ValueError(f"Manager Agent {manager_agent_id} n√£o encontrado")

                specialist_agents_data = [a for a in agents_data if a.get('id') != manager_agent_id and a.get('isActive', True)]

                # BUSCAR KB ANTES da delega√ß√£o (para todos os agentes que tem KB configurada)
                # Juntar todas as KBs de todos os agentes (manager + specialists)
                all_kb_ids = set()
                for agent in [manager_agent_data] + specialist_agents_data:
                    if agent.get('useKnowledgeBase'):
                        kb_ids = agent.get('knowledgeBaseIds', [])
                        all_kb_ids.update(kb_ids)

                knowledge_context = None
                if all_kb_ids:
                    print(f"üìö Buscando Knowledge Base ANTES da delega√ß√£o...")
                    try:
                        team_id_for_kb = str(team_definition.get('id', 'playground'))
                        kb_chunks = self.knowledge_service.search_knowledge(
                            team_id=team_id_for_kb,
                            document_ids=list(all_kb_ids),
                            query=task,
                            top_k=20
                        )

                        if kb_chunks:
                            knowledge_context = "\n\n".join([
                                f"üìÑ {chunk['metadata'].get('filename', 'Documento')}: {chunk['content']}"
                                for chunk in kb_chunks
                            ])
                            print(f"‚úÖ {len(kb_chunks)} chunks encontrados ANTES da delega√ß√£o")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erro ao buscar KB: {e}")

                # Chamar delega√ß√£o hier√°rquica manual COM knowledge_context
                delegation_result = self._run_manual_hierarchical_delegation(
                    message=task,
                    manager_agent_data=manager_agent_data,
                    specialist_agents_data=specialist_agents_data,
                    conversation_history=formatted_history,
                    llm=custom_llm,
                    knowledge_context=knowledge_context
                )

                response_text = delegation_result.get('response', '')
                agent_used = delegation_result.get('agent_used', 'Unknown')

                # Buscar o agente que realmente respondeu (pode ser o manager ou um especialista)
                delegated_agent_name = delegation_result.get('delegation_info', {}).get('delegated_to')
                selected_agent_data = next((a for a in agents_data if a.get('name') == delegated_agent_name), manager_agent_data)
            else:
                # MODO SEQUENTIAL: Usar keyword matching com manuten√ß√£o de contexto
                selected_agent_data = self._select_agent_by_keywords(
                    task,
                    agents_data,
                    team_definition.get("defaultAgentId"),
                    conversation_history=formatted_history
                )
                if not selected_agent_data:
                    selected_agent_data = next((a for a in agents_data if a.get('isActive', True)), agents_data[0])

                agent_used = selected_agent_data.get('name', 'Agente')

            print(f"‚úÖ Agente selecionado: {agent_used}")

            # Buscar Knowledge Base APENAS para modo SEQUENTIAL
            # (no modo hierarchical j√° foi buscado antes da delega√ß√£o)
            if process_type != 'hierarchical':
                knowledge_context = None
                if selected_agent_data.get('useKnowledgeBase'):
                    kb_ids = selected_agent_data.get('knowledgeBaseIds', [])
                    if kb_ids:
                        print(f"üìö Buscando Knowledge Base...")
                        try:
                            # Usar teamId da defini√ß√£o se existir
                            team_id_for_kb = str(team_definition.get('id', 'playground'))
                            kb_chunks = self.knowledge_service.search_knowledge(
                                team_id=team_id_for_kb,
                                document_ids=kb_ids,
                                query=task,
                                top_k=20
                            )

                            if kb_chunks:
                                knowledge_context = "\n\n".join([
                                    f"üìÑ {chunk['metadata'].get('filename', 'Documento')}: {chunk['content']}"
                                    for chunk in kb_chunks
                                ])
                                print(f"‚úÖ {len(kb_chunks)} chunks encontrados")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erro ao buscar KB (n√£o cr√≠tico no playground): {e}")

            # Gerar resposta
            start_time = time.time()

            # Vari√°veis para prompt e exemplos
            prompt_used = ""
            training_examples_used = []

            # S√≥ gerar resposta se N√ÉO for hierarchical (que j√° gerou)
            if process_type != 'hierarchical':
                response_text, prompt_used, training_examples_used = self._create_simple_response(
                    task,
                    selected_agent_data,
                    formatted_history,  # Hist√≥rico de conversa√ß√£o para contexto
                    custom_llm,
                    knowledge_context
                )
            elapsed_time = time.time() - start_time

            # Restaurar stdout
            sys.stdout = original_stdout
            execution_logs = log_capture.getvalue()

            success = True

            print(f"‚úÖ Resposta gerada em {elapsed_time:.2f}s")
            print(f"üìù Logs capturados: {len(execution_logs)} caracteres")

            # Debug: verificar agent_id
            agent_id_value = selected_agent_data.get('id')
            print(f"üîç DEBUG AGENT_ID - selected_agent_data.keys(): {selected_agent_data.keys()}")
            print(f"üîç DEBUG AGENT_ID - agent_id value: {agent_id_value}")
            print(f"üîç DEBUG AGENT_ID - agent_used: {agent_used}")

            return {
                "success": True,
                "final_output": response_text,
                "execution_logs": execution_logs,
                "agent_used": agent_used,
                "agent_id": agent_id_value,
                "config_used": {
                    "process_type": process_type,
                    "temperature": temperature,
                    "agent_name": agent_used
                },
                "processing_time": round(elapsed_time, 2),
                "prompt_used": prompt_used,
                "training_examples_used": training_examples_used,
                "training_examples_count": len(training_examples_used)
            }

        except Exception as e:
            # Restaurar stdout em caso de erro
            sys.stdout = original_stdout
            execution_logs = log_capture.getvalue()

            print(f"‚ùå Erro no playground: {e}")
            import traceback
            traceback.print_exc()

            error_message = str(e)

            return {
                "success": False,
                "final_output": f"Erro ao executar playground: {error_message}",
                "execution_logs": execution_logs + f"\n\nERRO: {error_message}",
                "error": error_message,
                "agent_used": agent_used,
                "processing_time": 0
            }

    async def process_message(
        self,
        tenant_id: str,
        crew_id: str,
        message: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        team_data: Optional[Dict[str, Any]] = None,
        agent_override: Optional[str] = None,
        remote_jid: Optional[str] = None,
        contact_id: Optional[int] = None,
        ticket_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Processa mensagem usando configura√ß√µes avan√ßadas da equipe"""
        
        print("\n" + "="*60)
        print("üéØ PROCESSANDO MENSAGEM - CrewAI Real Engine")
        print("="*60)
        print(f"Tenant ID: {tenant_id}")
        print(f"Crew ID: {crew_id}")
        print(f"Mensagem: {message}")
        print(f"Hist√≥rico recebido: {len(conversation_history) if conversation_history else 0} mensagens")
        if conversation_history:
            print(f"Exemplo do hist√≥rico: {conversation_history[0]}")
        print("="*60 + "\n")

        success = False
        response_text = ""
        error_message = None
        selected_agent_data = None
        prompt_used = ""

        try:
            if not self.llm:
                error_message = "LLM n√£o inicializado"
                return {
                    "success": False,
                    "response": "Desculpe, o servi√ßo de IA n√£o est√° dispon√≠vel no momento.",
                    "error": error_message
                }

            if not team_data:
                error_message = "No team data provided"
                return {
                    "success": False,
                    "response": "Desculpe, n√£o foi poss√≠vel processar sua mensagem. Equipe n√£o configurada.",
                    "error": error_message
                }

            agents = team_data.get('agents', [])
            if not agents:
                error_message = "No agents in team"
                return {
                    "success": False,
                    "response": "Desculpe, n√£o h√° agentes dispon√≠veis nesta equipe.",
                    "error": error_message
                }

            process_type = team_data.get('processType', 'sequential')
            temperature = team_data.get('temperature', 0.7)
            verbose = team_data.get('verbose', True)

            print(f"\n‚öôÔ∏è CONFIGURA√á√ïES DA EQUIPE:")
            print(f"   Process Type: {process_type}")
            print(f"   Temperature: {temperature}")
            print(f"   Verbose: {verbose}")
            print(f"   Total de agentes: {len(agents)}\n")
            # DEBUG: Mostrar configura√ß√£o de cada agente
            for idx, agent in enumerate(agents, 1):
                print(f"   Agente {idx}: {agent.get('name')}")
                print(f"      - useKnowledgeBase: {agent.get('useKnowledgeBase')}")
                if agent.get('knowledgeBaseIds'):
                    print(f"      - knowledgeBaseIds: {agent.get('knowledgeBaseIds')}")
            print()

            custom_llm = self._get_llm_for_team({
                'temperature': temperature,
                'processType': process_type,
                'managerLLM': team_data.get('managerLLM')
            })

            # MODO HIERARCHICAL: Delega√ß√£o manual - EXIGE Manager Agent configurado
            if process_type == 'hierarchical':
                manager_agent_id = team_data.get('managerAgentId')
                
                # EXIGIR managerAgentId configurado no dropdown
                if not manager_agent_id:
                    print(f"‚ùå Modo hierarchical mas nenhum Manager Agent foi selecionado no dropdown")
                    return {
                        "success": False,
                        "response": "Desculpe, a equipe n√£o est√° configurada corretamente. Por favor, selecione um Agente Coordenador (Manager) nas configura√ß√µes da equipe.",
                        "error": "Manager Agent not selected in team settings"
                    }
                
                print(f"üéØ Modo HIERARCHICAL - Manager Agent ID: {manager_agent_id}")
                
                # Encontrar Manager Agent na lista
                manager_agent_data = next((a for a in agents if a.get('id') == manager_agent_id), None)
                
                if not manager_agent_data:
                    print(f"‚ùå Manager Agent ID {manager_agent_id} n√£o encontrado na lista de agentes")
                    error_message = f"Manager Agent ID {manager_agent_id} not found"
                    return {
                        "success": False,
                        "response": "Desculpe, o agente coordenador n√£o foi encontrado.",
                        "error": error_message
                    }
                
                print(f"‚úÖ Manager Agent encontrado: {manager_agent_data.get('name')}")
                
                # Separar especialistas (todos os agentes exceto o manager)
                specialist_agents_data = [a for a in agents if a.get('id') != manager_agent_id and a.get('isActive', True)]
                print(f"üìã Especialistas dispon√≠veis: {len(specialist_agents_data)}")
                for specialist in specialist_agents_data:
                    print(f"   - {specialist.get('name')} ({specialist.get('function')})")
                
                # Buscar Knowledge Base (pode ser usado por qualquer agente)
                knowledge_context = None
                kb_chunks = []
                kb_usage_info = None
                
                # Verificar se algum agente usa KB
                agents_with_kb = [a for a in agents if a.get('useKnowledgeBase')]
                if agents_with_kb:
                    all_kb_ids = []
                    for agent in agents_with_kb:
                        kb_ids = agent.get('knowledgeBaseIds', [])
                        all_kb_ids.extend(kb_ids)
                    
                    all_kb_ids = list(set(all_kb_ids))  # Remove duplicates
                    
                    if all_kb_ids:
                        print(f"üìö Buscando Knowledge Base: {len(all_kb_ids)} documentos")
                        try:
                            kb_chunks = self.knowledge_service.search_knowledge(
                                team_id=str(crew_id),
                                document_ids=all_kb_ids,
                                query=message,
                                top_k=20
                            )

                            if kb_chunks:
                                knowledge_context = "\n\n".join([
                                    f"üìÑ {chunk['metadata'].get('filename', 'Documento')}: {chunk['content']}"
                                    for chunk in kb_chunks
                                ])
                                print(f"‚úÖ {len(kb_chunks)} chunks relevantes encontrados do KB")

                                kb_usage_info = {
                                    "used": True,
                                    "documentsSearched": len(all_kb_ids),
                                    "chunksFound": len(kb_chunks),
                                    "chunks": [
                                        {
                                            "filename": chunk['metadata'].get('filename', 'Documento'),
                                            "documentId": chunk.get('documentId'),
                                            "similarity": round(chunk.get('similarity', 0), 3),
                                            "contentPreview": chunk['content'][:100] + "..." if len(chunk['content']) > 100 else chunk['content']
                                        }
                                        for chunk in kb_chunks
                                    ]
                                }
                            else:
                                print("üì≠ Nenhum chunk relevante encontrado")
                                kb_usage_info = {
                                    "used": True,
                                    "documentsSearched": len(all_kb_ids),
                                    "chunksFound": 0,
                                    "chunks": []
                                }
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erro ao buscar KB: {e}")
                            kb_usage_info = {
                                "used": True,
                                "documentsSearched": len(all_kb_ids),
                                "chunksFound": 0,
                                "error": str(e)
                            }
                
                # Converter hist√≥rico para o formato esperado
                formatted_history = []
                if conversation_history:
                    for msg in conversation_history:
                        if msg.get('role') == 'user':
                            formatted_history.append({"sender": "user", "body": msg.get('content', '')})
                        elif msg.get('role') == 'assistant':
                            formatted_history.append({"sender": "assistant", "body": msg.get('content', '')})
                
                # Usar delega√ß√£o hier√°rquica com CrewAI Tasks
                print("üöÄ Iniciando delega√ß√£o hier√°rquica com CrewAI Tasks...")
                start_time = time.time()
                
                delegation_result = self._run_manual_hierarchical_delegation(
                    message=message,
                    manager_agent_data=manager_agent_data,
                    specialist_agents_data=specialist_agents_data,
                    conversation_history=formatted_history,
                    llm=custom_llm,
                    knowledge_context=knowledge_context
                )
                
                elapsed_time = time.time() - start_time
                
                if not delegation_result.get('success'):
                    return {
                        "success": False,
                        "response": delegation_result.get('response', 'Erro na delega√ß√£o'),
                        "error": delegation_result.get('error', 'Unknown delegation error')
                    }
                
                response_text = delegation_result['response']
                selected_agent_data = manager_agent_data  # Para logs
                success = True
                prompt_used = f"[Hierarchical Delegation] Manager: {manager_agent_data.get('name')}, Specialists: {len(specialist_agents_data)}"
                
                print(f"‚úÖ Delega√ß√£o conclu√≠da em {elapsed_time:.2f}s")
                
            else:
                # MODO SEQUENTIAL: Usar keyword matching com manuten√ß√£o de contexto
                selected_agent_data = self._select_agent_by_keywords(
                    message,
                    agents,
                    team_data.get("defaultAgentId"),
                    conversation_history=conversation_history
                )

                if not selected_agent_data:
                    error_message = "No appropriate agent found"
                    return {
                        "success": False,
                        "response": "Desculpe, n√£o consegui encontrar um agente apropriado para sua solicita√ß√£o.",
                        "error": error_message
                    }

                print(f"‚úÖ Usando agente: {selected_agent_data.get('name')}")

                # Buscar Knowledge Base se o agente usar
                knowledge_context = None
                kb_chunks = []
                kb_usage_info = None

                if selected_agent_data.get('useKnowledgeBase'):
                    kb_ids = selected_agent_data.get('knowledgeBaseIds', [])
                    if kb_ids:
                        print(f"üìö Buscando Knowledge Base: {len(kb_ids)} documentos")
                        try:
                            kb_chunks = self.knowledge_service.search_knowledge(
                                team_id=str(crew_id),
                                document_ids=kb_ids,
                                query=message,
                                top_k=20
                            )

                            if kb_chunks:
                                knowledge_context = "\n\n".join([
                                    f"üìÑ {chunk['metadata'].get('filename', 'Documento')}: {chunk['content']}"
                                    for chunk in kb_chunks
                                ])
                                print(f"‚úÖ {len(kb_chunks)} chunks relevantes encontrados do KB")

                                kb_usage_info = {
                                    "used": True,
                                    "documentsSearched": len(kb_ids),
                                    "chunksFound": len(kb_chunks),
                                    "chunks": [
                                        {
                                            "filename": chunk['metadata'].get('filename', 'Documento'),
                                            "documentId": chunk.get('documentId'),
                                            "similarity": round(chunk.get('similarity', 0), 3),
                                            "contentPreview": chunk['content'][:100] + "..." if len(chunk['content']) > 100 else chunk['content']
                                        }
                                        for chunk in kb_chunks
                                    ]
                                }
                            else:
                                print("üì≠ Nenhum chunk relevante encontrado")
                                kb_usage_info = {
                                    "used": True,
                                    "documentsSearched": len(kb_ids),
                                    "chunksFound": 0,
                                    "chunks": []
                                }
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erro ao buscar KB: {e}")
                            kb_usage_info = {
                                "used": True,
                                "documentsSearched": len(kb_ids),
                                "chunksFound": 0,
                                "error": str(e)
                            }

                print("üöÄ Gerando resposta com Vertex AI...")
                start_time = time.time()

                response_text, prompt_used, training_examples_used = self._create_simple_response(
                    message,
                    selected_agent_data,
                    conversation_history or [],
                    custom_llm,
                    knowledge_context
                )

                elapsed_time = time.time() - start_time
                success = True

            print(f"‚úÖ Resposta gerada em {elapsed_time:.2f}s")

            # Salvar log no backend
            log_data = {
                "companyId": int(tenant_id),
                "teamId": int(crew_id),
                "agentId": selected_agent_data.get('id'),
                "message": message,
                "response": response_text,
                "agentConfig": {
                    "name": selected_agent_data.get('name'),
                    "function": selected_agent_data.get('function'),
                    "objective": selected_agent_data.get('objective'),
                    "keywords": selected_agent_data.get('keywords', []),
                    "useKnowledgeBase": selected_agent_data.get('useKnowledgeBase', False),
                    "trainingExamplesUsed": len(training_examples_used),
                    "trainingExamples": [
                        {
                            "feedbackType": ex.get('feedbackType'),
                            "priority": ex.get('priority'),
                            "userMessage": ex.get('userMessage', '')[:100],  # Preview
                            "hasCorrection": bool(ex.get('correctedResponse'))
                        }
                        for ex in training_examples_used
                    ]
                },
                "knowledgeBaseUsage": kb_usage_info,
                "teamConfig": {
                    "processType": process_type,
                    "temperature": temperature,
                    "verbose": verbose
                },
                "promptUsed": prompt_used,
                "processingTime": round(elapsed_time, 2),
                "success": True,
                "errorMessage": None
            }
            
            self._save_log_to_backend(log_data)

            return {
                "success": True,
                "response": response_text,
                "agent_used": selected_agent_data.get('name'),
                "processing_time": round(elapsed_time, 2),
                "config_used": {
                    "process_type": process_type,
                    "temperature": temperature,
                    "verbose": verbose
                }
            }

        except Exception as e:
            print(f"‚ùå Erro ao processar mensagem: {e}")
            import traceback
            traceback.print_exc()
            error_message = str(e)
            
            # Salvar log de erro
            if selected_agent_data and team_data:
                log_data = {
                    "companyId": int(tenant_id),
                    "teamId": int(crew_id),
                    "agentId": selected_agent_data.get('id'),
                    "message": message,
                    "response": "Erro ao processar",
                    "agentConfig": {"name": selected_agent_data.get('name')},
                    "teamConfig": {"processType": team_data.get('processType', 'sequential')},
                    "promptUsed": prompt_used if prompt_used else None,
                    "processingTime": 0,
                    "success": False,
                    "errorMessage": error_message
                }
                self._save_log_to_backend(log_data)
            
            return {
                "success": False,
                "response": "Desculpe, ocorreu um erro ao processar sua mensagem. Por favor, tente novamente.",
                "error": error_message
            }
